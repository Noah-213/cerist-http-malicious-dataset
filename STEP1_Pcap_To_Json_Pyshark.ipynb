{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPUwrkIZ8FAsLMJApG0PbUW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhiKClh1Et30","executionInfo":{"status":"ok","timestamp":1749231415986,"user_tz":-60,"elapsed":19924,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}},"outputId":"4afcd4fd-5004-4231-d43b-bd7d3fc433ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n","Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# --- Unmount any existing Drive mount (if needed) ---\n","drive.flush_and_unmount()\n","\n","# --- (Optional) Clear local /content/drive mountpoint (disabled for safety) ---\n","drive_path = '/content/drive'\n","if os.path.exists(drive_path) and os.listdir(drive_path):\n","    print(\"Notice: /content/drive is not empty. Skipping automatic deletion for safety.\")\n","    # Uncomment below to forcibly clear the mountpoint (NOT your actual Drive!)\n","    # !rm -rf /content/drive/*\n","\n","# --- Mount Google Drive ---\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","\n","# --- Define input and output directories (adjust as needed) ---\n","PCAP_DIR = \"/content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/CERIST_RAW_DATA/attack_classes_pcap\"\n","ENRICHED_JSON_DIR = \"/content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark\"\n","\n","# --- Create output directory ---\n","os.makedirs(ENRICHED_JSON_DIR, exist_ok=True)"],"metadata":{"id":"QlnBOWuJFBmV","executionInfo":{"status":"ok","timestamp":1749231983603,"user_tz":-60,"elapsed":56,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# --- Install system dependency: TShark (required by PyShark) ---\n","!apt-get update\n","!apt-get install -y tshark\n","\n","# --- Allow TShark (dumpcap) to capture without root password ---\n","!chmod +x /usr/bin/dumpcap  # Be cautious: ensures executable permission only\n","\n","# --- Install Python dependencies ---\n","!pip install pyshark\n","!pip install nest_asyncio\n","\n","# --- Import libraries ---\n","import json\n","import pyshark\n","import nest_asyncio\n","from os import listdir\n","from os.path import join\n","\n","# --- Enable nested event loop support (required in Colab) ---\n","nest_asyncio.apply()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aehp9TGLIloi","executionInfo":{"status":"ok","timestamp":1749231995104,"user_tz":-60,"elapsed":10064,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}},"outputId":"d82cad9e-d8b8-46b4-edfb-c5887ee1c13d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,295 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n","Fetched 5,103 kB in 3s (1,687 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tshark is already the newest version (3.6.2-2).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Requirement already satisfied: pyshark in /usr/local/lib/python3.11/dist-packages (0.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pyshark) (5.4.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyshark) (3.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyshark) (24.2)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pyshark) (1.4.4)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"]}]},{"cell_type":"code","source":["pcap_file_path = \"/content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/CERIST_RAW_DATA/attack_classes_pcap/wfuzz_lfi.pcap\"\n","all_http_fields = set()\n","\n","try:\n","    # Use PyShark to capture packets from the specific file\n","    cap = pyshark.FileCapture(pcap_file_path)\n","    # Iterate through each packet\n","    for pkt in cap:\n","        # Check if the packet has the HTTP layer and the _all_fields attribute\n","        if hasattr(pkt, 'http') and hasattr(pkt.http, '_all_fields'):\n","             all_http_fields.update(pkt.http._all_fields.keys())\n","    # Close the capture file\n","    cap.close()\n","\n","except Exception as e:\n","    print(f\"An error occurred while processing the pcap file: {e}\")\n","\n","\n","print(all_http_fields)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGzR00kMc6SO","executionInfo":{"status":"ok","timestamp":1749231995111,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}},"outputId":"f8cf1197-7e93-494e-e3e2-a30836de74e2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["An error occurred while processing the pcap file: [Errno 2] No such file or directory: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/CERIST_RAW_DATA/attack_classes_pcap/wfuzz_xss.pcap\n","set()\n"]}]},{"cell_type":"code","source":["import json\n","import pyshark\n","\n","# Only these fields will be kept from _all_fields\n","SELECTED_HTTP_FIELDS = {\n","    '', 'http.time', 'http.request_number', '_ws.expert.severity', 'http.request.method',\n","    'http.prev_response_in', 'http.connection', 'http.request',\n","    '_ws.expert.message', 'http.prev_request_in', 'http.response', 'http.request.uri',\n","    '_ws.expert.group', 'http.accept', 'http.request_in', 'http.response.phrase',\n","    'http.file_data', 'http.server', 'http.chat', 'http.host', 'http.request.line',\n","    'http.response_number', 'http.date', 'http.response_for.uri', 'http.content_type',\n","    'http.user_agent', 'http.response.code.desc', 'http.content_length',\n","    'http.response.version', 'http.request.version', '_ws.expert', 'http.response.line',\n","    'http.response.code', 'http.request.full_uri'\n","}\n","\n","def extract_http_sessions_selected_fields(pcap_path, attack_tag, output_path, max_errors=5):\n","    \"\"\"\n","    Extracts HTTP request/response information using only the specified fields.\n","    \"\"\"\n","    cap = pyshark.FileCapture(pcap_path, display_filter=\"http\")\n","    current_streams = {}\n","    error_count = 0\n","\n","    for pkt in cap:\n","        try:\n","            stream_id = int(pkt.tcp.stream)\n","            is_request = hasattr(pkt.http, \"request_method\")\n","            is_response = hasattr(pkt.http, \"response_code\")\n","\n","            def filter_fields(all_fields):\n","                # Filter only the selected fields and return as dict\n","                return {k: v for k, v in all_fields.items() if k in SELECTED_HTTP_FIELDS}\n","\n","            # Initialize or retrieve the HTTP stream record\n","            entry = current_streams.get(stream_id, {\n","                \"attack_tag\": attack_tag,\n","                \"src_ip\": pkt.ip.src,\n","                \"dst_ip\": pkt.ip.dst,\n","                \"src_port\": pkt.tcp.srcport,\n","                \"dst_port\": pkt.tcp.dstport,\n","                \"req_method\": None,\n","                \"req_url\": None,\n","                \"req_headers\": {},\n","                \"req_body\": None,\n","                \"res_headers\": {},\n","                \"res_body\": None\n","            })\n","\n","            if is_request:\n","                entry[\"req_method\"] = getattr(pkt.http, \"request_method\", None)\n","                entry[\"req_url\"] = getattr(pkt.http, \"request_full_uri\", None)\n","                entry[\"req_headers\"] = filter_fields(dict(pkt.http._all_fields))\n","                entry[\"req_body\"] = getattr(pkt.http, \"file_data\", None)\n","\n","            if is_response:\n","                entry[\"res_headers\"] = filter_fields(dict(pkt.http._all_fields))\n","                entry[\"res_body\"] = getattr(pkt.http, \"file_data\", None)\n","\n","            current_streams[stream_id] = entry\n","\n","        except Exception as e:\n","            if error_count < max_errors:\n","                print(f\"[!] Skipping packet due to error: {e}\")\n","                error_count += 1\n","            continue\n","\n","    cap.close()\n","\n","    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(list(current_streams.values()), f, indent=2)\n"],"metadata":{"id":"HhD8-RhzGCCR","executionInfo":{"status":"ok","timestamp":1749232014061,"user_tz":-60,"elapsed":17,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# --- Batch process all .pcap files using PyShark ---\n","for file in os.listdir(PCAP_DIR):\n","    if file.endswith(\".pcap\"):\n","        # Derive attack type from filename\n","        attack_tag = file.replace(\"wfuzz_\", \"\").replace(\".pcap\", \"\")\n","\n","        # Construct input and output paths\n","        pcap_path = os.path.join(PCAP_DIR, file)\n","        output_path = os.path.join(ENRICHED_JSON_DIR, f\"{attack_tag}_pyshark.json\")\n","\n","        # Process the file\n","        print(f\"\\n[*] Processing file: {file}\")\n","        extract_http_sessions_selected_fields(pcap_path, attack_tag, output_path)\n","        print(f\"[✔] Enriched HTTP sessions saved to: {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4_yB9X-HNQs","executionInfo":{"status":"ok","timestamp":1749247312062,"user_tz":-60,"elapsed":519387,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}},"outputId":"8e04e7ce-674a-45df-a557-7e201e159a8b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[*] Processing file: wfuzz_xml.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/xml_pyshark.json\n","\n","[*] Processing file: wfuzz_lfi.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/lfi_pyshark.json\n","\n","[*] Processing file: wfuzz_ssti.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/ssti_pyshark.json\n","\n","[*] Processing file: wfuzz_cmdinj.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/cmdinj_pyshark.json\n","\n","[*] Processing file: wfuzz_sql_2.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/sql_2_pyshark.json\n","\n","[*] Processing file: wfuzz_sql_3.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/sql_3_pyshark.json\n","\n","[*] Processing file: wfuzz_sql_1.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/sql_1_pyshark.json\n","\n","[*] Processing file: wfuzz_xss_1.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/xss_1_pyshark.json\n","\n","[*] Processing file: wfuzz_xss_2.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/xss_2_pyshark.json\n","\n","[*] Processing file: wfuzz_xss_3.pcap\n","[✔] Enriched HTTP sessions saved to: /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/xss_3_pyshark.json\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import json\n","\n","def combine_and_cleanup(directory, attack_prefix):\n","    \"\"\"\n","    Combine tous les fichiers JSON du dossier 'directory' commençant par 'attack_prefix'\n","    dans un fichier '{attack_prefix}_pyshark.json', puis supprime les fichiers sources SAUF le fichier combiné.\n","    Tous les champs 'attack_tag' sont réécrits pour correspondre à 'attack_prefix'.\n","    \"\"\"\n","    pattern = os.path.join(directory, f\"{attack_prefix}*.json\")\n","    file_list = glob.glob(pattern)\n","    combined_data = []\n","\n","    for filename in file_list:\n","        with open(filename, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","            if isinstance(data, list):\n","                combined_data.extend(data)\n","            else:\n","                combined_data.append(data)\n","\n","    # ---- Normalisation du tag ----\n","    for entry in combined_data:\n","        entry['attack_tag'] = attack_prefix\n","\n","    # Création du fichier combiné\n","    output_file = os.path.join(directory, f\"{attack_prefix}_pyshark.json\")\n","    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(combined_data, f, indent=2, ensure_ascii=False)\n","\n","    # Suppression des fichiers source, sauf le fichier combiné !\n","    for filename in file_list:\n","        # On compare les chemins absolus pour éviter tout piège\n","        if os.path.abspath(filename) == os.path.abspath(output_file):\n","            continue  # Ne pas supprimer le fichier combiné !\n","        try:\n","            os.remove(filename)\n","            print(f\"Supprimé : {filename}\")\n","        except Exception as e:\n","            print(f\"Erreur lors de la suppression de {filename} : {e}\")\n","\n","    print(f\"\\nFichier créé : {output_file} ({len(combined_data)} lignes)\\n\")\n","\n","# Exemple d'appel :\n","combine_and_cleanup('/content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/', 'xss')\n","combine_and_cleanup('/content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/', 'sql')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_x3xghhldb5U","executionInfo":{"status":"ok","timestamp":1749247934475,"user_tz":-60,"elapsed":62,"user":{"displayName":"Tarek Bennouar","userId":"05753099953120326546"}},"outputId":"d8297428-2098-473d-b3e1-1f2cd2e578b2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Fichier créé : /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/xss_pyshark.json (154 lignes)\n","\n","\n","Fichier créé : /content/drive/MyDrive/BINOME_WORK/STAGE_CERIST/DATA_FORMATING/STEP1_json_enriched_pyshark/sql_pyshark.json (171 lignes)\n","\n"]}]}]}